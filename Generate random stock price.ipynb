{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.backend import clear_session\n",
    "from keras import backend as K\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from colour import Color\n",
    "from gym import spaces, logger\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class trading_env:\n",
    "    def __init__(self, obs_data_len, step_len,\n",
    "                 df, fee, max_position=5, deal_col_name='price', plot_col_name='middle',\n",
    "                 feature_names=['price', 'volume'], \n",
    "                 return_transaction=True,\n",
    "                 fluc_div=100.0, gameover_limit=5,\n",
    "                 *args, **kwargs):\n",
    "\n",
    "        self.df = df\n",
    "        self.action_space = 3\n",
    "        # self.action_space = spaces.Discrete(3)\n",
    "        self.action_describe = {0:'do nothing',  1:'long', 2:'short'}\n",
    "        \n",
    "        self.obs_len = obs_data_len\n",
    "        # self.feature_len = len(feature_names)\n",
    "        self.feature_len = len(feature_names)+8  # return_transaction\n",
    "\n",
    "        # self.observation_space = np.array([self.obs_len*self.feature_len,])\n",
    "        # self.observation_space = np.array([self.obs_len*self.feature_len,])\n",
    "        # print(self.feature_len)\n",
    "        # print(self.obs_len)\n",
    "        # print(self.observation_space.shape)\n",
    "\n",
    "        self.using_feature = feature_names\n",
    "        self.price_name = deal_col_name\n",
    "        self.price_plot = plot_col_name\n",
    "        \n",
    "        self.step_len = step_len\n",
    "        self.fee = fee\n",
    "        self.max_position = max_position\n",
    "        \n",
    "        self.fluc_div = fluc_div\n",
    "        self.gameover = gameover_limit\n",
    "        self.return_transaction = return_transaction\n",
    "        \n",
    "        self.begin_fs = self.df[self.df['serial_number']==0]\n",
    "        self.date_leng = len(self.begin_fs)\n",
    "        \n",
    "        self.render_on = 0\n",
    "        self.buy_color, self.sell_color = (1, 2)\n",
    "        self.new_rotation, self.cover_rotation = (1, 2)\n",
    "        self.transaction_details = pd.DataFrame()\n",
    "    \n",
    "    def _random_choice_section(self):\n",
    "        random_int = np.random.randint(self.date_leng)\n",
    "        if random_int == self.date_leng - 1:\n",
    "            begin_point = self.begin_fs.index[random_int]\n",
    "            end_point = None\n",
    "        else:\n",
    "            begin_point, end_point = self.begin_fs.index[random_int: random_int+2]\n",
    "        df_section = self.df.iloc[begin_point: end_point]\n",
    "        return df_section\n",
    "\n",
    "    def reset(self):\n",
    "        self.df_sample = self._random_choice_section()\n",
    "        self.step_st = 0\n",
    "\n",
    "        # define the price to calculate the reward\n",
    "        self.price = self.df_sample[self.price_name].as_matrix()\n",
    "        # define the price to plot BB\n",
    "        # self.price_mid = self.df_sample[self.price_plot].as_matrix()\n",
    "\n",
    "        # define the observation feature\n",
    "        self.obs_features = self.df_sample[self.using_feature].as_matrix()\n",
    "        \n",
    "        # maybe make market position feature in final feature, set as option\n",
    "        self.posi_arr = np.zeros_like(self.price)\n",
    "        # position variation\n",
    "        self.posi_variation_arr = np.zeros_like(self.posi_arr)\n",
    "        # position entry or cover :new_entry->1  increase->2 cover->-1 decrease->-2\n",
    "        self.posi_entry_cover_arr = np.zeros_like(self.posi_arr)\n",
    "        # self.position_feature = np.array(self.posi_l[self.step_st:self.step_st+self.obs_len])/(self.max_position*2)+0.5\n",
    "        \n",
    "        self.price_mean_arr = self.price.copy()\n",
    "        self.reward_fluctuant_arr = (self.price - self.price_mean_arr)*self.posi_arr\n",
    "        self.reward_makereal_arr = self.posi_arr.copy()\n",
    "        self.reward_arr = self.reward_fluctuant_arr*self.reward_makereal_arr\n",
    "\n",
    "        self.info = None\n",
    "        self.transaction_details = pd.DataFrame()\n",
    "        \n",
    "        # observation part\n",
    "        # state = xdata[0:1, :]\n",
    "        # self.obs_state = self.obs_features[self.step_st: self.step_st+self.obs_len ]\n",
    "        self.obs_state = self.obs_features[self.step_st: self.step_st+self.obs_len]\n",
    "\n",
    "\n",
    "        self.obs_posi = self.posi_arr[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_posi_var = self.posi_variation_arr[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_posi_entry_cover = self.posi_entry_cover_arr[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_price = self.price[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_price_mean = self.price_mean_arr[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_reward_fluctuant = self.reward_fluctuant_arr[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_makereal = self.reward_makereal_arr[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_reward = self.reward_arr[self.step_st: self.step_st+self.obs_len]\n",
    "        \n",
    "        if self.return_transaction:\n",
    "            self.obs_return = np.concatenate((self.obs_state, \n",
    "                                            self.obs_posi[:, np.newaxis], \n",
    "                                            self.obs_posi_var[:, np.newaxis],\n",
    "                                            self.obs_posi_entry_cover[:, np.newaxis],\n",
    "                                            self.obs_price[:, np.newaxis],\n",
    "                                            self.obs_price_mean[:, np.newaxis],\n",
    "                                            self.obs_reward_fluctuant[:, np.newaxis],\n",
    "                                            self.obs_makereal[:, np.newaxis],\n",
    "                                            self.obs_reward[:, np.newaxis]), axis=1)\n",
    "        else:\n",
    "            self.obs_return = self.obs_state\n",
    "\n",
    "        # # Convert Tuple to list\n",
    "        # self.obs_return = [list(i) for i in self.obs_return]\n",
    "\n",
    "        self.t_index = 0\n",
    "        return self.obs_return\n",
    "    \n",
    "    def _long(self, open_posi, enter_price, current_mkt_position, current_price_mean):\n",
    "        if open_posi:\n",
    "            self.chg_price_mean[:] = enter_price\n",
    "            self.chg_posi[:] = 1\n",
    "            self.chg_posi_var[:1] = 1\n",
    "            self.chg_posi_entry_cover[:1] = 1\n",
    "        else:\n",
    "            after_act_mkt_position = current_mkt_position + 1\n",
    "            self.chg_price_mean[:] = (current_price_mean*current_mkt_position + \\\n",
    "                                        enter_price)/after_act_mkt_position\n",
    "            self.chg_posi[:] = after_act_mkt_position\n",
    "            self.chg_posi_var[:1] = 1\n",
    "            self.chg_posi_entry_cover[:1] = 2\n",
    "            \n",
    "    def _short(self, open_posi, enter_price, current_mkt_position, current_price_mean):\n",
    "        if open_posi:\n",
    "            self.chg_price_mean[:] = enter_price\n",
    "            self.chg_posi[:] = -1\n",
    "            self.chg_posi_var[:1] = -1\n",
    "            self.chg_posi_entry_cover[:1] = 1\n",
    "        else:\n",
    "            after_act_mkt_position = current_mkt_position - 1\n",
    "            self.chg_price_mean[:] = (current_price_mean*abs(current_mkt_position) + \\\n",
    "                                      enter_price)/abs(after_act_mkt_position)\n",
    "            self.chg_posi[:] = after_act_mkt_position\n",
    "            self.chg_posi_var[:1] = -1\n",
    "            self.chg_posi_entry_cover[:1] = 2\n",
    "          \n",
    "    def _short_cover(self, current_price_mean, current_mkt_position):\n",
    "        self.chg_price_mean[:] = current_price_mean\n",
    "        self.chg_posi[:] = current_mkt_position + 1\n",
    "        self.chg_makereal[:1] = 1\n",
    "        self.chg_reward[:] = ((self.chg_price - self.chg_price_mean)*(-1) - self.fee)*self.chg_makereal\n",
    "        self.chg_posi_var[:1] = 1\n",
    "        self.chg_posi_entry_cover[:1] = -1\n",
    "    \n",
    "    def _long_cover(self, current_price_mean, current_mkt_position):\n",
    "        self.chg_price_mean[:] = current_price_mean\n",
    "        self.chg_posi[:] = current_mkt_position - 1\n",
    "        self.chg_makereal[:1] = 1\n",
    "        self.chg_reward[:] = ((self.chg_price - self.chg_price_mean)*(1) - self.fee)*self.chg_makereal\n",
    "        self.chg_posi_var[:1] = -1\n",
    "        self.chg_posi_entry_cover[:1] = -1\n",
    "    \n",
    "    def _stayon(self, current_price_mean, current_mkt_position):\n",
    "        self.chg_posi[:] = current_mkt_position\n",
    "        self.chg_price_mean[:] = current_price_mean\n",
    "\n",
    "    def step(self, action):\n",
    "        current_index = self.step_st + self.obs_len -1\n",
    "        current_price_mean = self.price_mean_arr[current_index]\n",
    "        current_mkt_position = self.posi_arr[current_index]\n",
    "\n",
    "        self.t_index += 1\n",
    "        self.step_st += self.step_len\n",
    "\n",
    "        # observation part\n",
    "        self.obs_state = self.obs_features[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_posi = self.posi_arr[self.step_st: self.step_st+self.obs_len]\n",
    "\n",
    "        # position variation\n",
    "        self.obs_posi_var = self.posi_variation_arr[self.step_st: self.step_st+self.obs_len]\n",
    "\n",
    "        # position entry or cover :new_entry->1  increase->2 cover->-1 decrease->-2\n",
    "        self.obs_posi_entry_cover = self.posi_entry_cover_arr[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_price = self.price[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_price_mean = self.price_mean_arr[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_reward_fluctuant = self.reward_fluctuant_arr[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_makereal = self.reward_makereal_arr[self.step_st: self.step_st+self.obs_len]\n",
    "        self.obs_reward = self.reward_arr[self.step_st: self.step_st+self.obs_len]\n",
    "\n",
    "        # change part\n",
    "        self.chg_posi = self.obs_posi[-self.step_len:]\n",
    "        self.chg_posi_var = self.obs_posi_var[-self.step_len:]\n",
    "        self.chg_posi_entry_cover = self.obs_posi_entry_cover[-self.step_len:]\n",
    "        self.chg_price = self.obs_price[-self.step_len:]\n",
    "        self.chg_price_mean = self.obs_price_mean[-self.step_len:]\n",
    "        self.chg_reward_fluctuant = self.obs_reward_fluctuant[-self.step_len:]\n",
    "        self.chg_makereal = self.obs_makereal[-self.step_len:]\n",
    "        self.chg_reward = self.obs_reward[-self.step_len:]\n",
    "\n",
    "        done = False\n",
    "        if self.step_st+self.obs_len+self.step_len >= len(self.price):\n",
    "            done = True\n",
    "            action = -1\n",
    "            if current_mkt_position != 0:\n",
    "                self.chg_price_mean[:] = current_price_mean\n",
    "                self.chg_posi[:] = 0\n",
    "                self.chg_posi_var[:1] = -current_mkt_position\n",
    "                self.chg_posi_entry_cover[:1] = -2\n",
    "                self.chg_makereal[:1] = 1\n",
    "                self.chg_reward[:] = ((self.chg_price - self.chg_price_mean)*(current_mkt_position) - abs(current_mkt_position)*self.fee)*self.chg_makereal\n",
    "\n",
    "            self.transaction_details = pd.DataFrame([self.posi_arr,\n",
    "                                                     self.posi_variation_arr,\n",
    "                                                     self.posi_entry_cover_arr,\n",
    "                                                     self.price_mean_arr,\n",
    "                                                     self.reward_fluctuant_arr,\n",
    "                                                     self.reward_makereal_arr,\n",
    "                                                     self.reward_arr], \n",
    "                                                     index=['position', 'position_variation', 'entry_cover',\n",
    "                                                            'price_mean', 'reward_fluctuant', 'reward_makereal',\n",
    "                                                            'reward'], \n",
    "                                                     columns=self.df_sample.index).T\n",
    "\n",
    "            self.info = self.df_sample.join(self.transaction_details)  # Original\n",
    "\n",
    "            \n",
    "        # use next tick, maybe choice avg in first 10 tick will be better to real backtest\n",
    "        enter_price = self.chg_price[0]\n",
    "        if action == 1 and self.max_position > current_mkt_position >= 0:\n",
    "            open_posi = (current_mkt_position == 0)\n",
    "            self._long(open_posi, enter_price, current_mkt_position, current_price_mean)\n",
    "        \n",
    "        elif action == 2 and -self.max_position < current_mkt_position <= 0:\n",
    "            open_posi = (current_mkt_position == 0)\n",
    "            self._short(open_posi, enter_price, current_mkt_position, current_price_mean)\n",
    "        \n",
    "        elif action == 1 and current_mkt_position < 0:\n",
    "            self._short_cover(current_price_mean, current_mkt_position)\n",
    "\n",
    "        elif action == 2 and current_mkt_position>0:\n",
    "            self._long_cover(current_price_mean, current_mkt_position)\n",
    "\n",
    "        elif action == 1 and current_mkt_position==self.max_position:\n",
    "            action = 0\n",
    "\n",
    "        elif action == 2 and current_mkt_position==-self.max_position:\n",
    "            action = 0\n",
    "        \n",
    "        if action == 0:\n",
    "            if current_mkt_position != 0:\n",
    "                self._stayon(current_price_mean, current_mkt_position)\n",
    "\n",
    "        self.chg_reward_fluctuant[:] = (self.chg_price - self.chg_price_mean)*self.chg_posi - np.abs(self.chg_posi)*self.fee\n",
    "\n",
    "        if self.return_transaction:\n",
    "            self.obs_return = np.concatenate((self.obs_state, \n",
    "                                            self.obs_posi[:, np.newaxis],                  # 1\n",
    "                                            self.obs_posi_var[:, np.newaxis],              # 2\n",
    "                                            self.obs_posi_entry_cover[:, np.newaxis],      # 3\n",
    "                                            self.obs_price[:, np.newaxis],                 # 4\n",
    "                                            self.obs_price_mean[:, np.newaxis],            # 5\n",
    "                                            self.obs_reward_fluctuant[:, np.newaxis],      # 6\n",
    "                                            self.obs_makereal[:, np.newaxis],              # 7\n",
    "                                            self.obs_reward[:, np.newaxis]), axis=1)       # 8\n",
    "        else:\n",
    "            self.obs_return = self.obs_state\n",
    "\n",
    "        # return self.obs_return, self.obs_reward.sum(), done, self.info   # Original\n",
    "        # return self.obs_return , self.obs_reward[0], done ,self.obs_reward_fluctuant[0] ,self.info\n",
    "        # return self.obs_return , self.obs_reward[0], done ,self.obs_reward_fluctuant[0] ,self.reward_arr[:self.step_st+self.obs_len].cumsum()[-1] ,self.info\n",
    "        return self.obs_return , self.obs_reward[0], done ,self.obs_reward_fluctuant[0] ,self.info\n",
    "\n",
    "    def _gen_trade_color(self, ind, long_entry=(0, 1, 0, 0.8), long_cover=(1, 1, 1, 0.5), \n",
    "                         short_entry=(1, 0, 0, 0.8), short_cover=(1, 1, 1, 0.5)): \n",
    "        if self.posi_variation_arr[ind]>0 and self.posi_entry_cover_arr[ind]>0:\n",
    "            return long_entry\n",
    "        elif self.posi_variation_arr[ind]>0 and self.posi_entry_cover_arr[ind]<0:\n",
    "            return long_cover\n",
    "        elif self.posi_variation_arr[ind]<0 and self.posi_entry_cover_arr[ind]>0:\n",
    "            return short_entry\n",
    "        elif self.posi_variation_arr[ind]<0 and self.posi_entry_cover_arr[ind]<0:\n",
    "            return short_cover \n",
    "    \n",
    "    def _plot_trading(self):\n",
    "        price_x = list(range(len(self.price[:self.step_st+self.obs_len])))\n",
    "        # price_ma = list(range(len(self.price_mid[:self.step_st+self.obs_len])))\n",
    "\n",
    "        self.price_plot = self.ax.plot(price_x, self.price[:self.step_st+self.obs_len], c=(0, 0.68, 0.95, 0.9),zorder=1)\n",
    "        # self.price_plot2 = self.ax.plot(price_ma, self.price_mid[:self.step_st+self.obs_len], c=(0.5, 0, 0, 0.2),zorder=1)\n",
    "\n",
    "        # maybe seperate up down color\n",
    "        #self.price_plot = self.ax.plot(price_x, self.price[:self.step_st+self.obs_len], c=(0, 0.75, 0.95, 0.9),zorder=1)\n",
    "        # self.features_plot = [self.ax3.plot(price_x, self.obs_features[:self.step_st+self.obs_len, i], \n",
    "        #                                     c=self.features_color[i])[0] for i in range(self.feature_len)]\n",
    "        rect_high = self.obs_price.max() - self.obs_price.min()\n",
    "        self.target_box = self.ax.add_patch(\n",
    "                            patches.Rectangle(\n",
    "                            (self.step_st, self.obs_price.min()), self.obs_len, rect_high,\n",
    "                            label='observation',edgecolor=(0.9, 1, 0.2, 0.8),facecolor=(0.95,1,0.1,0.1),\n",
    "                            linestyle='-',linewidth=1.5,\n",
    "                            fill=True)\n",
    "                            )     # remove background)\n",
    "        self.fluc_reward_plot_p = self.ax2.fill_between(price_x, 0, self.reward_fluctuant_arr[:self.step_st+self.obs_len],\n",
    "                                                        where=self.reward_fluctuant_arr[:self.step_st+self.obs_len]>=0, \n",
    "                                                        facecolor=(1, 0.8, 0, 0.2), edgecolor=(1, 0.8, 0, 0.9), linewidth=0.8)\n",
    "        self.fluc_reward_plot_n = self.ax2.fill_between(price_x, 0, self.reward_fluctuant_arr[:self.step_st+self.obs_len],\n",
    "                                                        where=self.reward_fluctuant_arr[:self.step_st+self.obs_len]<=0, \n",
    "                                                        facecolor=(0, 1, 0.8, 0.2), edgecolor=(0, 1, 0.8, 0.9), linewidth=0.8)\n",
    "        self.posi_plot_long = self.ax3.fill_between(price_x, 0, self.posi_arr[:self.step_st+self.obs_len], \n",
    "                                                    where=self.posi_arr[:self.step_st+self.obs_len]>=0, \n",
    "                                                    facecolor=(1, 0.5, 0, 0.2), edgecolor=(1, 0.5, 0, 0.9), linewidth=1)\n",
    "        self.posi_plot_short = self.ax3.fill_between(price_x, 0, self.posi_arr[:self.step_st+self.obs_len], \n",
    "                                                     where=self.posi_arr[:self.step_st+self.obs_len]<=0, \n",
    "                                                     facecolor=(0, 0.5, 1, 0.2), edgecolor=(0, 0.5, 1, 0.9), linewidth=1)\n",
    "        self.reward_plot_p = self.ax2.fill_between(price_x, 0, \n",
    "                                                   self.reward_arr[:self.step_st+self.obs_len].cumsum(),\n",
    "                                                   where=self.reward_arr[:self.step_st+self.obs_len].cumsum()>=0,\n",
    "                                                   facecolor=(0, 1, 0, 0.2), edgecolor=(0, 1, 0, 0.9), linewidth=1)\n",
    "        self.reward_plot_n = self.ax2.fill_between(price_x, 0, \n",
    "                                                   self.reward_arr[:self.step_st+self.obs_len].cumsum(),\n",
    "                                                   where=self.reward_arr[:self.step_st+self.obs_len].cumsum()<=0,\n",
    "                                                   facecolor=(1, 0, 0, 0.2), edgecolor=(1, 0, 0, 0.9), linewidth=1)\n",
    "\n",
    "        trade_x = self.posi_variation_arr.nonzero()[0]\n",
    "        trade_x_buy = [i for i in trade_x if self.posi_variation_arr[i]>0]\n",
    "        trade_x_sell = [i for i in trade_x if self.posi_variation_arr[i]<0]\n",
    "        trade_y_buy = [self.price[i] for i in trade_x_buy]\n",
    "        trade_y_sell =  [self.price[i] for i in trade_x_sell]\n",
    "        trade_color_buy = [self._gen_trade_color(i) for i in trade_x_buy] \n",
    "        trade_color_sell = [self._gen_trade_color(i) for i in trade_x_sell]\n",
    "        self.trade_plot_buy = self.ax.scatter(x=trade_x_buy, y=trade_y_buy, s=100, marker='^', \n",
    "                                              c=trade_color_buy, edgecolors=(0,1,0,0.4), zorder=2)\n",
    "        self.trade_plot_sell = self.ax.scatter(x=trade_x_sell, y=trade_y_sell, s=100, marker='v', \n",
    "                                               c=trade_color_sell, edgecolors=(1,0,0,0.4), zorder=2)\n",
    "\n",
    "    def render(self, save=False):\n",
    "        if self.render_on == 0:\n",
    "            matplotlib.style.use('dark_background')\n",
    "            self.render_on = 1\n",
    "\n",
    "            left, width = 0.1, 0.8\n",
    "            rect1 = [left, 0.42, width, 0.55]\n",
    "            rect2 = [left, 0.12, width, 0.3]\n",
    "            rect3 = [left, 0.01, width, 0.11]\n",
    "\n",
    "            self.fig = plt.figure(figsize=(15,8))\n",
    "            self.fig.suptitle('%s'%self.df_sample['datetime'].iloc[0].date(), fontsize=14, fontweight='bold')\n",
    "            # self.ax = self.fig.add_subplot(1,1,1)\n",
    "            self.ax = self.fig.add_axes(rect1)  # left, bottom, width, height\n",
    "            self.ax2 = self.fig.add_axes(rect2, sharex=self.ax)\n",
    "            self.ax3 = self.fig.add_axes(rect3, sharex=self.ax)\n",
    "            self.ax.grid(color='gray', linestyle='-', linewidth=0.5)\n",
    "            self.ax2.grid(color='gray', linestyle='-', linewidth=0.5)\n",
    "            self.ax3.grid(color='gray', linestyle='-', linewidth=0.5)\n",
    "            self.features_color = [c.rgb+(0.9,) for c in Color('yellow').range_to(Color('cyan'), self.feature_len)]\n",
    "            #fig, ax = plt.subplots()\n",
    "            self._plot_trading()\n",
    "\n",
    "            self.ax.set_xlim(0,len(self.price[:self.step_st+self.obs_len])+200)\n",
    "            plt.ion()\n",
    "            #self.fig.tight_layout()\n",
    "            plt.show()\n",
    "            if save:\n",
    "                self.fig.savefig('fig/%s.png' % str(self.t_index))\n",
    "\n",
    "        elif self.render_on == 1:\n",
    "            self.ax.lines.remove(self.price_plot[0])\n",
    "            # [self.ax3.lines.remove(plot) for plot in self.features_plot]\n",
    "            self.fluc_reward_plot_p.remove()\n",
    "            self.fluc_reward_plot_n.remove()\n",
    "            self.target_box.remove()\n",
    "            self.reward_plot_p.remove()\n",
    "            self.reward_plot_n.remove()\n",
    "            self.posi_plot_long.remove()\n",
    "            self.posi_plot_short.remove()\n",
    "            self.trade_plot_buy.remove()\n",
    "            self.trade_plot_sell.remove()\n",
    "\n",
    "            self._plot_trading()\n",
    "\n",
    "            self.ax.set_xlim(0,len(self.price[:self.step_st+self.obs_len])+200)\n",
    "            if save:\n",
    "                self.fig.savefig('fig/%s.png' % str(self.t_index))\n",
    "            plt.pause(0.0001)\n",
    "  \n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 2000\n",
    "memory_max = 2000\n",
    "\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the Double DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 256 #64\n",
    "        self.train_start = 1000\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/cartpole_ddqn.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.state_size+3, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.add(Dense(self.state_size+3, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min :\n",
    "            self.epsilon -= (1.0/EPISODES)/250\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_next = self.model.predict(update_target)\n",
    "        target_val = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_next[i])\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    target_val[i][a])\n",
    "\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,epochs=1, verbose=0)\n",
    "        # K.clear_session()\n",
    "\n",
    "        \n",
    "\n",
    "def feature(fename) :\n",
    "    full = ['Open','High','Low','Close','ma2','ma3','ma4','ma5','ma10','ma50','ma200','macd','macdsignal','macdhist','rsi','upper','middle','lower',\n",
    "                'smin','smax','lmin','lmax','rcloseopen','rclosehigh','rcloselow','rma5ma10','rma10ma50','rma50ma200','rma5upper','rma5middle',\n",
    "                'rma5lowwer','rma5smin','rma5smax','rma5lmin','rma5lmax','rrsi'] #,'rcloseopen1','rclosehigh1','rcloselow1', 'rma5ma101','rma10ma501',\n",
    "#                 'rma50ma2001','rma5upper1','rma5middle1','rma5lowwer1','rma5smin1','rma5smax1','rma5lmin1','rma5lmax1','rrsi1']\n",
    "\n",
    "    full_price = ['Open','High','Low','Close','ma2','ma3','ma4','ma5','ma10','ma50','ma200','upper','middle','lower',\n",
    "                    'smin','smax','lmin','lmax']\n",
    "    \n",
    "    full_relative = ['rcloseopen','rclosehigh','rcloselow','rma5ma10','rma10ma50','rma50ma200','rma5upper','rma5middle',\n",
    "                    'rma5lowwer','rma5smin','rma5smax','rma5lmin','rma5lmax','rrsi']\n",
    "\n",
    "    full_relative_shift = ['rcloseopen','rclosehigh','rcloselow','rma5ma10','rma10ma50','rma50ma200','rma5upper','rma5middle',\n",
    "                    'rma5lowwer','rma5smin','rma5smax','rma5lmin','rma5lmax','rrsi','rcloseopen1','rclosehigh1','rcloselow1',\n",
    "                    'rma5ma101','rma10ma501','rma50ma2001','rma5upper1','rma5middle1','rma5lowwer1','rma5smin1','rma5smax1',\n",
    "                    'rma5lmin1','rma5lmax1','rrsi1']\n",
    "\n",
    "    if fename == 'fullRelative' :\n",
    "        feature_name_list = full_relative\n",
    "    elif fename == 'fullPrice' :\n",
    "        feature_name_list = full_price\n",
    "    elif fename == 'fullRelativeShift' :\n",
    "        feature_name_list = full_relative_shift\n",
    "    else : feature_name_list = full\n",
    "\n",
    "    return feature_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Q-table:\n",
      "\n",
      "       left     right\n",
      "0  0.000000  0.004320\n",
      "1  0.000000  0.025005\n",
      "2  0.000030  0.111241\n",
      "3  0.000000  0.368750\n",
      "4  0.027621  0.745813\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df =  pd.read_csv('dataset\\SET_wINDICATOR.csv' )\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "    # fename , state_size , action_size = 'fullRelative' , 12 , 3\n",
    "    # fename , state_size , action_size= 'fullPrice'  , 26  ,3\n",
    "    # fename , state_size , action_size = 'fullRelativeShift' , 36 , 3\n",
    "    fename , state_size , action_size = 'full' , 58 , 3\n",
    "\n",
    "    Obs_data_len , Step_len, Max_position = 1,5,5\n",
    "\n",
    "    env = trading_env(obs_data_len=Obs_data_len, step_len=Step_len, max_position=Max_position,\n",
    "                        df=df, fee=0.1, deal_col_name='Close', plot_col_name='middle',\n",
    "                        feature_names=feature(fename=fename) ,return_transaction = True)\n",
    "                        \n",
    "    agent = DoubleDQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes, mean_scores = [], [], []\n",
    "    # max_reward = 0\n",
    "    \n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        t3 = time.time()\n",
    "\n",
    "        # # Validation data\n",
    "        # start_split = np.random.randint(0,1300)\n",
    "        # df1 =df.iloc[start_split : start_split+1400]\n",
    "        # df1['serial_number'] = range(0,len(df1))\n",
    "        # print(df1.shape)\n",
    "\n",
    "        # env = trading_env(obs_data_len=Obs_data_len, step_len=Step_len, max_position=Max_position,\n",
    "        #                 df=df1, fee=0.1, deal_col_name='Close', plot_col_name='middle',\n",
    "        #                 feature_names=feature(fename=fename) ,return_transaction=True)\n",
    "\n",
    "        done = False\n",
    "        score = 0\n",
    "        sc ,nr ,rf ,s = [] , [] , [] , 0\n",
    "        reward1 = 0\n",
    "\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, reward_fluc, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            reward1 = reward\n",
    "            # Drawdowm from negative fluc\n",
    "            if reward_fluc < 0 :\n",
    "                reward1 = -10\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward1, next_state, done )\n",
    "            agent.train_model()   # every time step do the training\n",
    "            state = next_state\n",
    "            \n",
    "            score += reward   # For collect to \n",
    "            net_reward = score + reward_fluc\n",
    "            sc.append(score)\n",
    "            nr.append(net_reward)\n",
    "            rf.append(reward_fluc)\n",
    "            s += 1\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.update_target_model()\n",
    "                \n",
    "                score += reward\n",
    "                net_reward = score + reward_fluc\n",
    "                sc.append(score)\n",
    "                nr.append(net_reward)\n",
    "                rf.append(reward_fluc)\n",
    "                s += 1\n",
    "                \n",
    "                # every episode, plot the play time\n",
    "                episodes.append(e)\n",
    "                scores.append(score)\n",
    "                mean_scores.append(np.mean(scores[-100:]) if len(scores) > 100 else 0)\n",
    "\n",
    "                \n",
    "                if e > (EPISODES-50) :\n",
    "                    fig ,ax1 = plt.subplots()\n",
    "                    ax1.plot(range(0,s), sc, 'b')\n",
    "                    ax1.plot(range(0,s), nr, 'r',\",\")\n",
    "                    ax1.plot(range(0,s), rf, 'g',\",\")\n",
    "                    ax1.grid(True)                    \n",
    "                    fig.savefig(\"cartpole/2-double-dqn/save_graph/ddqnVAL_FP256_{},{:0.0f}.png\".format(e,score))\n",
    "                    plt.close('all')\n",
    "                    agent.model.save_weights(\"cartpole/2-double-dqn/save_model/ddqnVAL_FP256_{},{:0.0f}.h5\".format(e,score))\n",
    "\n",
    "        # Plot Graph Learning Progress\n",
    "        pylab.plot(episodes, scores, 'b')\n",
    "        pylab.plot(episodes, mean_scores, 'r')\n",
    "        pylab.grid(True)\n",
    "        pylab.savefig(\"cartpole/2-double-dqn/save_graph/a_DDQN_FP256_{:.0f}.png\".format(EPISODES))\n",
    "        plt.close('all')\n",
    "\n",
    "        t4 = (time.time()-t3)\n",
    "        t5 = time.ctime(time.time()+((EPISODES-e)*t4))\n",
    "        print(\"Epoch: %s    Reward: %0.2f    Epsilon: %0.2f     %0.2f sec    %s\" % (e,score,agent.epsilon,t4,t5))\n",
    "        print()\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
